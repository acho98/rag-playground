{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced Retrieval With LangChain\n",
        "\n",
        "- Multi Query - Given a single user query, use an LLM to synthetically generate multiple other queries. Use each one of the new queries to retrieve documents, take the union of those documents for the final context of your prompt\n",
        "\n",
        "- Contextual Compression - Fluff remover. Normal retrieval but with an extra step of pulling out relevant information from each returned document. This makes each relevant document smaller for your final prompt (which increases information density)\n",
        "\n",
        "- Parent Document Retriever - Split and embed small chunks (for maximum information density), then return the parent documents (or larger chunks) those small chunks come from\n",
        "\n",
        "- Ensemble Retriever - Combine multiple retrievers together\n",
        "\n",
        "- Self-Query - When the retriever infers filters from a users query and applies those filters to the underlying data"
      ],
      "metadata": {
        "id": "GX7r0TTYSr_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "!pip install langchain_openai\n",
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install unstructured\n",
        "!pip install chromadb\n",
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moZ91Qd8TOYF",
        "outputId": "72670034-642e-42d0-e4bd-df106e8aa0c5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.9)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.34.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.80)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.9)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.80)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain) (3.0.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.9)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.80)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain-community) (2.7.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain-community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain-community) (2.18.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.14.6)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.23.7)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.6.2)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (24.1)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured) (4.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.1)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.3-py3-none-any.whl (559 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.7.3)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.4)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.3)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.3.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading email_validator-2.1.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12.1)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (7.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.23.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=ca21b65e813ca67644343f6e4ad0bee132c0184a31cc21c616d1648047000973\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, uvicorn, ujson, python-multipart, overrides, opentelemetry-util-http, opentelemetry-proto, humanfriendly, httptools, dnspython, deprecated, chroma-hnswlib, bcrypt, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, email_validator, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation-asgi, fastapi-cli, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, fastapi, chromadb\n",
            "Successfully installed asgiref-3.8.1 bcrypt-4.1.3 chroma-hnswlib-0.7.3 chromadb-0.5.3 coloredlogs-15.0.1 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.1.2 fastapi-0.111.0 fastapi-cli-0.0.4 httptools-0.6.1 humanfriendly-10.0 kubernetes-30.1.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.18.0 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-grpc-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 overrides-7.7.0 posthog-3.5.0 pypika-0.48.9 python-multipart-0.0.9 starlette-0.37.2 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from dotenv import load_dotenv\n",
        "# import os\n",
        "\n",
        "# load_dotenv()\n",
        "OpenAI_key=\"your api key\""
      ],
      "metadata": {
        "id": "ApyxgqjHS6bE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings,ChatOpenAI"
      ],
      "metadata": {
        "id": "kGCZDn5PTI2_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_dir = \"/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/\"\n",
        "\n",
        "# loader = DirectoryLoader('../data/PaulGrahamEssaysLarge/', glob=\"**/*.txt\", show_progress=True)\n",
        "loader = DirectoryLoader(file_dir, glob=\"**/*.txt\", show_progress=True)\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OalhZ-EfX9e1",
        "outputId": "42f0b4d4-1cbe-4c47-b764-3e3afe0dcbf1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/49 [00:00<?, ?it/s]\u001b[A[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "\n",
            "  2%|▏         | 1/49 [00:09<07:57,  9.95s/it]\u001b[A\n",
            "  4%|▍         | 2/49 [00:11<03:42,  4.74s/it]\u001b[A\n",
            "  6%|▌         | 3/49 [00:11<02:10,  2.83s/it]\u001b[A\n",
            "  8%|▊         | 4/49 [00:12<01:31,  2.03s/it]\u001b[A\n",
            " 10%|█         | 5/49 [00:13<01:09,  1.57s/it]\u001b[A\n",
            " 12%|█▏        | 6/49 [00:14<00:58,  1.35s/it]\u001b[A\n",
            " 14%|█▍        | 7/49 [00:15<00:52,  1.24s/it]\u001b[A\n",
            " 16%|█▋        | 8/49 [00:16<00:46,  1.15s/it]\u001b[A\n",
            " 18%|█▊        | 9/49 [00:16<00:38,  1.05it/s]\u001b[A\n",
            " 20%|██        | 10/49 [00:17<00:33,  1.16it/s]\u001b[A\n",
            " 22%|██▏       | 11/49 [00:17<00:28,  1.32it/s]\u001b[A\n",
            " 24%|██▍       | 12/49 [00:21<00:58,  1.59s/it]\u001b[A\n",
            " 27%|██▋       | 13/49 [00:21<00:45,  1.27s/it]\u001b[A\n",
            " 29%|██▊       | 14/49 [00:22<00:35,  1.03s/it]\u001b[A\n",
            " 31%|███       | 15/49 [00:23<00:34,  1.01s/it]\u001b[A\n",
            " 33%|███▎      | 16/49 [00:23<00:27,  1.18it/s]\u001b[A\n",
            " 35%|███▍      | 17/49 [00:24<00:23,  1.36it/s]\u001b[A\n",
            " 37%|███▋      | 18/49 [00:24<00:23,  1.33it/s]\u001b[A\n",
            " 39%|███▉      | 19/49 [00:25<00:24,  1.21it/s]\u001b[A\n",
            " 41%|████      | 20/49 [00:27<00:26,  1.10it/s]\u001b[A\n",
            " 43%|████▎     | 21/49 [00:27<00:20,  1.36it/s]\u001b[A\n",
            " 45%|████▍     | 22/49 [00:28<00:19,  1.36it/s]\u001b[A\n",
            " 47%|████▋     | 23/49 [00:28<00:16,  1.56it/s]\u001b[A\n",
            " 49%|████▉     | 24/49 [00:29<00:14,  1.69it/s]\u001b[A\n",
            " 51%|█████     | 25/49 [00:29<00:14,  1.70it/s]\u001b[A\n",
            " 53%|█████▎    | 26/49 [00:29<00:11,  1.95it/s]\u001b[A\n",
            " 55%|█████▌    | 27/49 [00:30<00:12,  1.79it/s]\u001b[A\n",
            " 57%|█████▋    | 28/49 [00:31<00:14,  1.47it/s]\u001b[A\n",
            " 59%|█████▉    | 29/49 [00:33<00:22,  1.12s/it]\u001b[A\n",
            " 61%|██████    | 30/49 [00:34<00:21,  1.15s/it]\u001b[A\n",
            " 63%|██████▎   | 31/49 [00:35<00:19,  1.10s/it]\u001b[A\n",
            " 65%|██████▌   | 32/49 [00:37<00:18,  1.10s/it]\u001b[A\n",
            " 67%|██████▋   | 33/49 [00:37<00:15,  1.03it/s]\u001b[A\n",
            " 69%|██████▉   | 34/49 [00:38<00:12,  1.23it/s]\u001b[A\n",
            " 71%|███████▏  | 35/49 [00:39<00:12,  1.10it/s]\u001b[A\n",
            " 73%|███████▎  | 36/49 [00:39<00:10,  1.27it/s]\u001b[A\n",
            " 76%|███████▌  | 37/49 [00:40<00:08,  1.47it/s]\u001b[A\n",
            " 78%|███████▊  | 38/49 [00:40<00:06,  1.58it/s]\u001b[A\n",
            " 80%|███████▉  | 39/49 [00:41<00:06,  1.65it/s]\u001b[A\n",
            " 82%|████████▏ | 40/49 [00:41<00:05,  1.57it/s]\u001b[A\n",
            " 84%|████████▎ | 41/49 [00:42<00:04,  1.76it/s]\u001b[A\n",
            " 86%|████████▌ | 42/49 [00:43<00:04,  1.69it/s]\u001b[A\n",
            " 88%|████████▊ | 43/49 [00:43<00:03,  1.67it/s]\u001b[A\n",
            " 90%|████████▉ | 44/49 [00:44<00:03,  1.51it/s]\u001b[A\n",
            " 92%|█████████▏| 45/49 [00:45<00:02,  1.44it/s]\u001b[A\n",
            " 94%|█████████▍| 46/49 [00:45<00:02,  1.43it/s]\u001b[A\n",
            " 96%|█████████▌| 47/49 [00:47<00:01,  1.21it/s]\u001b[A\n",
            " 98%|█████████▊| 48/49 [00:48<00:00,  1.15it/s]\u001b[A\n",
            "100%|██████████| 49/49 [00:48<00:00,  1.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"You have {len(docs)} essays loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKFntLx-TrV3",
        "outputId": "c6df68c8-ba2a-4b15-814a-6660f65b4949"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 49 essays loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print (f\"Your {len(docs)} documents have been split into {len(splits)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Zg7N1NYefe",
        "outputId": "58a8b167-152a-4d0a-b26a-d70a53589df2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your 49 documents have been split into 468 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'vectordb' in globals(): # If you've already made your vectordb this will delete it so you start fresh\n",
        "    vectordb.delete_collection()\n",
        "\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OpenAI_key)\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)"
      ],
      "metadata": {
        "id": "CHR4CWLgfje9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiQuery\n",
        "This retrieval method will generated 3 additional questions to get a total of 4 queries (with the users included) that will be used to go retrieve documents. This is helpful when you want to retrieve documents which are similar in meaning to your question."
      ],
      "metadata": {
        "id": "SwskvxVEhiwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.prompts import PromptTemplate\n",
        "# Set logging for the queries\n",
        "import logging"
      ],
      "metadata": {
        "id": "55PY_vqFfu79"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "kTA4ae7Chst9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the authors view on the early stages of a startup?\"\n",
        "llm = ChatOpenAI(openai_api_key=OpenAI_key, temperature=0)\n",
        "\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "dWuyBuQShx_2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7NEvKiHh76w",
        "outputId": "f6aef301-7d07-496f-d21a-bc6241b1ad0b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How does the author perceive the initial phases of a startup?', \"2. What are the author's thoughts on the beginning stages of a startup?\", '3. What insights does the author offer regarding the early development of a startup?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLlIW7qCiBD1",
        "outputId": "e460672d-b10f-4e21-f849-6f786b424529"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "CKPimOWqiE7G"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict(text=PROMPT.format_prompt(\n",
        "    context=unique_docs,\n",
        "    question=question\n",
        ").text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "NBFEvu8OiI7n",
        "outputId": "e14550fe-1d3a-490d-d794-8a178ad75aa6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The author's view on the early stages of a startup is that it is important to release a minimal version 1 of the product quickly and then improve it based on user reactions. The author emphasizes the importance of releasing early and not being too slow to release products.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contextual Compression\n",
        "Then we'll move onto contextual compression. This will take the chunk that you've made (above) and compress it's information down to the parts relevant to your query.\n",
        "\n",
        "Say that you have a chunk that has 3 topics within it, you only really care about one of them though, this compressor will look at your query, see that you only need one of the 3 topics, then extract & return that one topic.\n",
        "\n",
        "This one is a bit more expensive because each doc returned will get processed an additional time (to pull out the relevant data)"
      ],
      "metadata": {
        "id": "TcXiQbxGiVVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor"
      ],
      "metadata": {
        "id": "iJ4MgDeHiPpM"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, model='gpt-4-turbo-2024-04-09', openai_api_key=OpenAI_key)\n",
        "\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
        "                                                       base_retriever=vectordb.as_retriever())"
      ],
      "metadata": {
        "id": "Wu4C9QkQigtW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "upX-2w4RikQ1",
        "outputId": "2d4447ff-76ff-4c72-b6e7-8a1d3d4f6633"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Want to start a startup? Get funded by\\n\\nY Combinator.\\n\\nJuly 2004(This essay is derived from a talk at Oscon 2004.)\\n\\nA few months ago I finished a new\\n\\nbook,\\n\\nand in reviews I keep\\n\\nnoticing words like \"provocative\\'\\' and \"controversial.\\'\\' To say\\n\\nnothing of \"idiotic. \\'\\'I didn\\'t mean to make the book controversial. I was trying to make\\n\\nit efficient. I didn\\'t want to waste people\\'s time telling them\\n\\nthings they already knew. It\\'s more efficient just to give them\\n\\nthe diffs. But I suppose that\\'s bound to yield an alarming book.EdisonsThere\\'s no controversy about which idea is most controversial:\\n\\nthe suggestion that variation in wealth might not be as big a\\n\\nproblem as we think.I didn\\'t say in the book that variation in wealth was in itself a\\n\\ngood thing. I said in some situations it might be a sign of good\\n\\nthings. A throbbing headache is not a good thing, but it can be\\n\\na sign of a good thing-- for example, that you\\'re recovering\\n\\nconsciousness after being hit on the head.Variation in wealth can be a sign of variation in productivity.\\n\\n(In a society of one, they\\'re identical.) And that\\n\\nis almost certainly a good thing: if your society has no variation\\n\\nin productivity, it\\'s probably not because everyone is Thomas\\n\\nEdison. It\\'s probably because you have no Thomas Edisons.In a low-tech society you don\\'t see much variation in productivity.\\n\\nIf you have a tribe of nomads collecting sticks for a fire, how\\n\\nmuch more productive is the best stick gatherer going to be than'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compressor.compress_documents(documents=[splits[0]], query=\"test for what you like to do\")\n",
        "# this is weird, needs more test for different queries\n",
        "# result should long document to shrter document with more dense information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4iojs89i0lV",
        "outputId": "5afb0dca-2ff6-4577-d0ed-97f3153990c1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the authors view on the early stages of a startup?\"\n",
        "compressed_docs = compression_retriever.get_relevant_documents(question)"
      ],
      "metadata": {
        "id": "PpHFrSO3i7A0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (len(compressed_docs))\n",
        "compressed_docs\n",
        "# anyways it looks like working, but needs to check the difference between compressed docs and normal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVTrJmKRjIWm",
        "outputId": "b949ef7f-1018-465b-cd5d-1dd0a6245739"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Startups can die from releasing something full of bugs, and not fixing them fast enough, but I don't know of any that died from releasing something stable but minimal very early, then promptly improving it. [2]\", metadata={'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/startuplessons.txt'}),\n",
              " Document(page_content='Release Early.The thing I probably repeat most is this recipe for a startup: get\\n\\na version 1 out fast, then improve it based on users\\' reactions.By \"release early\" I don\\'t mean you should release something full\\n\\nof bugs, but that you should release something minimal. Users hate\\n\\nbugs, but they don\\'t seem to mind a minimal version 1, if there\\'s\\n\\nmore coming soon.There are several reasons it pays to get version 1 done fast. One\\n\\nis that this is simply the right way to write software, whether for\\n\\na startup or not. I\\'ve been repeating that since 1993, and I haven\\'t seen much since to\\n\\ncontradict it. I\\'ve seen a lot of startups die because they were\\n\\ntoo slow to release stuff, and none because they were too quick.', metadata={'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/startuplessons.txt'}),\n",
              " Document(page_content=\"Starting a company is common, but starting a startup is rare.\\n\\nI've talked about the distinction between the two elsewhere, but\\n\\nessentially a startup is a new business designed for scale. Most\\n\\nnew businesses are service businesses and except in rare cases those\\n\\ndon't scale. [4]\", metadata={'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/hubs.txt'}),\n",
              " Document(page_content=\"the key stage in the life of a startup happens before they want that kind of space. The key stage is when they're three guys operating out of an apartment. Wherever the startup is when it gets funded, it will stay. The defining quality\", metadata={'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/siliconvalley.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "_JBkX7djjpX3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict(text=PROMPT.format_prompt(\n",
        "    context=compressed_docs,\n",
        "    question=question\n",
        ").text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "1kEaaJAAkRHu",
        "outputId": "1c872a8a-6474-49c2-f0d7-97dc2b8af202"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The author's view on the early stages of a startup emphasizes the importance of releasing a product early, even if it is minimal, rather than waiting to release a more complete version that might be full of bugs. The author advocates for getting a version 1 out quickly and then improving it based on user feedback. This approach is seen as beneficial because it allows startups to adapt and evolve based on actual user needs and reactions, rather than assumptions. The author believes that releasing something stable but minimal early on and then promptly improving it is less likely to lead to the failure of a startup compared to releasing something full of bugs without quick fixes. This strategy is described as the right way to write software, not just for startups but in general, and is reiterated as a recipe for startup success.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parent Document Retriever\n",
        "[LangChain](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/parent_document_retriever/) documentation does a great job describing this - my minor edits below:\n",
        "\n",
        "When you split your docs, you generally may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
        "\n",
        "But at the same time you may want to have information around those small chunks to keep context of the longer document.\n",
        "\n",
        "The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n",
        "\n",
        "Note that \"parent document\" refers to the document that a small chunk originated from. This can either be the whole raw document OR a larger chunk."
      ],
      "metadata": {
        "id": "eDXDZDJBkV7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore"
      ],
      "metadata": {
        "id": "gHXgOCrbkSuK"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This text splitter is used to create the child documents. They should be small chunk size.\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)"
      ],
      "metadata": {
        "id": "Q4pi7DAvk3Vl"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"return_full_documents\",\n",
        "    embedding_function=OpenAIEmbeddings(openai_api_key=OpenAI_key)\n",
        ")"
      ],
      "metadata": {
        "id": "7_EjY9Vxk50n"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ],
      "metadata": {
        "id": "XrCObZu_k81I"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will add the whole essays that we split above. We haven't chunked these essays yet, but the .add_documents will do the small chunking for us with the child_splitter above"
      ],
      "metadata": {
        "id": "x67KpG-ylLj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.add_documents(docs, ids=None)"
      ],
      "metadata": {
        "id": "MHE1D61tlFf0"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_docs = vectorstore.similarity_search(\"what is some investing advice?\")\n",
        "sub_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blEdKXyAlRHZ",
        "outputId": "4b6d61ea-daad-4b74-ac63-93c48e75abe8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"people there are rich, or expect to be when their options vest.\\n\\nOrdinary employees find it very hard to recommend an acquisition;\\n\\nit's just too annoying to see a bunch of twenty year olds get rich\\n\\nwhen you're still working for salary. Even if it's the right thing\\n\\nfor your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.\", metadata={'doc_id': '66742520-3cb8-4846-aec8-3f07a4888411', 'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/vcsqueeze.txt'}),\n",
              " Document(page_content=\"the product is expensive to develop or sell, or simply because\\n\\nthey're wasteful.If you're paying attention, you'll be asking at this point not just\\n\\nhow to avoid the fatal pinch, but how to avoid being default dead.\\n\\nThat one is easy: don't hire too fast. Hiring too fast is by far\\n\\nthe biggest killer of startups that raise money.\", metadata={'doc_id': '1d7f8f52-b3b5-4a01-81a0-8b538b134205', 'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/aord.txt'}),\n",
              " Document(page_content=\"[1]\\n\\nBut investors are so fickle that you can never\\n\\ndo more than start to count on them. Sometimes something about your\\n\\nbusiness will spook investors even if your growth is great. So no\\n\\nmatter how good your growth is, you can never safely treat fundraising\\n\\nas more than a plan A. You should always have a plan B as well: you\\n\\nshould know (as in write down) precisely what you'll need to do to\", metadata={'doc_id': '1d7f8f52-b3b5-4a01-81a0-8b538b134205', 'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/aord.txt'}),\n",
              " Document(page_content=\"head that it does in mine. And if you set off the alarms sufficiently\\n\\nearly, you may be able to avoid the fatal pinch.It would be safe to be default dead if you could count on investors\\n\\nsaving you. As a rule their interest is a function of\\n\\ngrowth. If you have steep revenue growth, say over 5x a year, you\\n\\ncan start to count on investors being interested even if you're not\\n\\nprofitable.\\n\\n[1]\", metadata={'doc_id': '1d7f8f52-b3b5-4a01-81a0-8b538b134205', 'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/aord.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(\"what is some investing advice?\")"
      ],
      "metadata": {
        "id": "Eh4B4hhJlXrg"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm going to only do the first doc to save space, but there are more waiting for you. Keep in mind that LangChain will do the union of docs, so if you have two child docs from the same parent doc, you'll only return the parent doc once, not twice."
      ],
      "metadata": {
        "id": "Z390z-a-lnX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs[0].page_content[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "2JgfO0FFleOD",
        "outputId": "e519b3e2-0b44-4bbe-a9c3-0c45ead9f800"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"November 2005In the next few years, venture capital funds will find themselves\\n\\nsqueezed from four directions. They're already stuck with a seller's\\n\\nmarket, because of the huge amounts they raised at the end of the\\n\\nBubble and still haven't invested. This by itself is not the end\\n\\nof the world. In fact, it's just a more extreme version of the\\n\\nnorm\\n\\nin the VC business: too much money chasing too few deals.Unfortunately, those few deals now want less and less money, because\\n\\nit's getting so cheap to start a startup. The four causes: open\\n\\nsource, which makes software free; Moore's law, which makes hardware\\n\\ngeometrically closer to free; the Web, which makes promotion free\\n\\nif you're good; and better languages, which make development a lot\\n\\ncheaper.When we started our startup in 1995, the first three were our biggest\\n\\nexpenses. We had to pay $5000 for the Netscape Commerce Server,\\n\\nthe only software that then supported secure http connections. We\\n\\npaid $3000 for a server with a 90 MHz p\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This text splitter is used to create the parent documents\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "\n",
        "# This text splitter is used to create the child documents\n",
        "# It should create documents smaller than the parent\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"return_split_parent_documents\", embedding_function=OpenAIEmbeddings(openai_api_key=OpenAI_key))\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()"
      ],
      "metadata": {
        "id": "lmE97tlPlog3"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter,\n",
        ")"
      ],
      "metadata": {
        "id": "03Cv6Rt4l_sf"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now this time when we add documents two things will happen\n",
        "\n",
        "1. Larger chunks - We'll split our docs into large chunks\n",
        "2. Smaller chunks - We'll split our docs into smaller chunks\n",
        "\n",
        "Both of them will be combined."
      ],
      "metadata": {
        "id": "NIQvGUQKmFs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.add_documents(docs)"
      ],
      "metadata": {
        "id": "Kx-Zczi0mChU"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(store.yield_keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS_MZ7_XmKT3",
        "outputId": "83586229-bedc-49bf-9c73-957c8671b969"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "385"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Then let's go get our small chunks to make sure it's working and see how long they are"
      ],
      "metadata": {
        "id": "kaqlqwZ4mf7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_docs = vectorstore.similarity_search(\"what is some investing advice?\")\n",
        "sub_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeiqsJRKmcO7",
        "outputId": "89712524-3d14-440b-af5b-f818a5fd46cd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"people there are rich, or expect to be when their options vest.\\n\\nOrdinary employees find it very hard to recommend an acquisition;\\n\\nit's just too annoying to see a bunch of twenty year olds get rich\\n\\nwhen you're still working for salary. Even if it's the right thing\\n\\nfor your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.\", metadata={'doc_id': '6e124873-b58e-45e4-85ae-7926d7d5456b', 'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/vcsqueeze.txt'}),\n",
              " Document(page_content=\"the product is expensive to develop or sell, or simply because\\n\\nthey're wasteful.If you're paying attention, you'll be asking at this point not just\\n\\nhow to avoid the fatal pinch, but how to avoid being default dead.\\n\\nThat one is easy: don't hire too fast. Hiring too fast is by far\\n\\nthe biggest killer of startups that raise money.\", metadata={'doc_id': 'f13982e6-27ce-400f-8f2d-b82983065bbf', 'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/aord.txt'}),\n",
              " Document(page_content=\"[1]\\n\\nBut investors are so fickle that you can never\\n\\ndo more than start to count on them. Sometimes something about your\\n\\nbusiness will spook investors even if your growth is great. So no\\n\\nmatter how good your growth is, you can never safely treat fundraising\\n\\nas more than a plan A. You should always have a plan B as well: you\\n\\nshould know (as in write down) precisely what you'll need to do to\", metadata={'doc_id': 'ff7d43d4-7e9d-4da9-938f-649db8f053cf', 'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/aord.txt'}),\n",
              " Document(page_content=\"head that it does in mine. And if you set off the alarms sufficiently\\n\\nearly, you may be able to avoid the fatal pinch.It would be safe to be default dead if you could count on investors\\n\\nsaving you. As a rule their interest is a function of\\n\\ngrowth. If you have steep revenue growth, say over 5x a year, you\\n\\ncan start to count on investors being interested even if you're not\\n\\nprofitable.\\n\\n[1]\", metadata={'doc_id': 'ff7d43d4-7e9d-4da9-938f-649db8f053cf', 'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/aord.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's do the full process, we'll see what small chunks are generated, but then return the larger chunks as our relevant documents"
      ],
      "metadata": {
        "id": "kDjEdVxMmg_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "larger_chunk_relevant_docs = retriever.get_relevant_documents(\"what is some investing advice?\")\n",
        "larger_chunk_relevant_docs[0]\n",
        "# Think how diffrence retrievals"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2GWYAxOmdLB",
        "outputId": "d8102eab-9cd0-4354-dac8-089a4dc9cfea"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='all practical purposes, succeeding now equals getting bought. Which\\n\\nmeans VCs are now in the business of finding promising little 2-3\\n\\nman startups and pumping them up into companies that cost $100\\n\\nmillion to acquire. They didn\\'t mean to be in this business; it\\'s\\n\\njust what their business has evolved into.Hence the fourth problem: the acquirers have begun to realize they\\n\\ncan buy wholesale. Why should they wait for VCs to make the startups\\n\\nthey want more expensive? Most of what the VCs add, acquirers don\\'t\\n\\nwant anyway. The acquirers already have brand recognition and HR\\n\\ndepartments. What they really want is the software and the developers,\\n\\nand that\\'s what the startup is in the early phase: concentrated\\n\\nsoftware and developers.Google, typically, seems to have been the first to figure this out.\\n\\n\"Bring us your startups early,\" said Google\\'s speaker at the Startup School. They\\'re quite\\n\\nexplicit about it: they like to acquire startups at just the point\\n\\nwhere they would do a Series A round. (The Series A round is the\\n\\nfirst round of real VC funding; it usually happens in the first\\n\\nyear.) It is a brilliant strategy, and one that other big technology\\n\\ncompanies will no doubt try to duplicate. Unless they want to have\\n\\nstill more of their lunch eaten by Google.Of course, Google has an advantage in buying startups: a lot of the\\n\\npeople there are rich, or expect to be when their options vest.\\n\\nOrdinary employees find it very hard to recommend an acquisition;\\n\\nit\\'s just too annoying to see a bunch of twenty year olds get rich\\n\\nwhen you\\'re still working for salary. Even if it\\'s the right thing\\n\\nfor your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.\\n\\nThey need to do two things, one of which won\\'t surprise them, and\\n\\nanother that will seem an anathema.Let\\'s start with the obvious one: lobby to get Sarbanes-Oxley\\n\\nloosened. This law was created to prevent future Enrons, not to', metadata={'source': '/content/drive/MyDrive/llm/RAG/playground/data/PaulGrahamEssaysLarge/vcsqueeze.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "question = \"what is some investing advice?\"\n",
        "\n",
        "llm.predict(text=PROMPT.format_prompt(\n",
        "    context=larger_chunk_relevant_docs,\n",
        "    question=question\n",
        ").text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "YfpdovIVmmki",
        "outputId": "9b3c6830-719a-46b6-f8cc-27185da96612"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The investing advice extracted from the provided documents includes:\\n\\n1. **Avoid Overhiring**: Startups should be cautious about hiring too quickly. Rapid hiring can be a significant factor leading to the failure of startups that have raised money. Founders often overestimate the need to hire to foster growth, which can lead to excessive spending and operational inefficiencies.\\n\\n2. **Focus on Product Appeal**: Ensure that the product is highly appealing to drive growth. Startups often fail because their product is only moderately appealing, leading to mediocre growth. Founders should focus on enhancing the product's appeal rather than assuming that hiring more employees will automatically boost growth.\\n\\n3. **Be Wary of Dependence on Further Funding**: Founders should not assume it will be easy to raise more money. It's crucial to distinguish between current facts and hopeful future outcomes. Startups should have a clear plan B for survival if additional funding cannot be secured, regardless of the current growth rate.\\n\\n4. **Manage Growth and Spending**: There is not necessarily a direct correlation between how much a startup spends and how fast it grows. Efficient management of resources can often lead to significant growth without excessive spending.\\n\\nThese pieces of advice highlight the importance of strategic planning, efficient resource management, and focusing on core product value as key factors in the success of startup investments.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Retriever\n",
        "The next one on our list combines multiple retrievers together. The goal here is to see what multiple methods return, then pull them together for (hopefully) better results.\n",
        "\n",
        "You may need to install bm25 with !pip install rank_bm25"
      ],
      "metadata": {
        "id": "EPxOdzxmnEDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
      ],
      "metadata": {
        "id": "tTjhnWfunBBp"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) retriever for this one which is really good at keyword matching (vs semantic). When you combine this method with regular semantic search it's known as hybrid search."
      ],
      "metadata": {
        "id": "EBwl-zYHnKjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the bm25 retriever and faiss retriever\n",
        "bm25_retriever = BM25Retriever.from_documents(splits)\n",
        "bm25_retriever.k = 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "4wNhIp1bnIc8",
        "outputId": "7dc35d32-2dc0-4729-b160-ac4309526cd7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Could not import rank_bm25, please install with `pip install rank_bm25`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/retrievers/bm25.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, metadatas, bm25_params, preprocess_func, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mrank_bm25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBM25Okapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rank_bm25'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-fb74e9b16938>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialize the bm25 retriever and faiss retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbm25_retriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBM25Retriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbm25_retriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/retrievers/bm25.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, bm25_params, preprocess_func, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[1;32m     90\u001b[0m         \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mbm25_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbm25_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/retrievers/bm25.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, metadatas, bm25_params, preprocess_func, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mrank_bm25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBM25Okapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0;34m\"Could not import rank_bm25, please install with `pip install \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;34m\"rank_bm25`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Could not import rank_bm25, please install with `pip install rank_bm25`.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjCkhQbmnUw-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}